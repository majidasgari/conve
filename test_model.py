#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ùˆ ØªØ³Øª Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ ConvE
"""

import json
import torch
import pickle
import numpy as np
import argparse
import sys
import os
import datetime

from evaluation import ranking_and_hits
from model import ConvE, DistMult, Complex

from spodernet.preprocessing.pipeline import Pipeline
from spodernet.utils.global_config import Config
from spodernet.preprocessing.batching import StreamBatcher

np.set_printoptions(precision=3)


def load_and_test_model(args):
    """
    ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù† Ùˆ ØªØ³Øª Ù…Ø¯Ù„
    """
    print("=" * 70)
    print("Ø´Ø±ÙˆØ¹ ØªØ³Øª Ù…Ø¯Ù„")
    print("=" * 70)
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ vocab
    input_keys = ['e1', 'rel', 'rel_eval', 'e2', 'e2_multi1', 'e2_multi2']
    p = Pipeline(args.data, keys=input_keys)
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ vocab ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ ÛŒØ§ Ù†Ù‡
    try:
        p.load_vocabs()
        vocab = p.state['vocab']
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ vocabulary: {e}")
        print("\nğŸ’¡ Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ dataset Ø±Ø§ preprocess Ú©Ù†ÛŒØ¯:")
        print(f"   python main.py --data {args.data} --preprocess")
        return
    
    num_entities = vocab['e1'].num_token
    num_relations = vocab['rel'].num_token
    
    # Ø¨Ø±Ø±Ø³ÛŒ ØµØ­Øª vocabulary
    if num_entities < 100 or num_relations < 10:
        print(f"\nâŒ Ø®Ø·Ø§: Vocabulary Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª!")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ Entities: {num_entities} (Ø®ÛŒÙ„ÛŒ Ú©Ù… Ø§Ø³Øª!)")
        print(f"   ØªØ¹Ø¯Ø§Ø¯ Relations: {num_relations} (Ø®ÛŒÙ„ÛŒ Ú©Ù… Ø§Ø³Øª!)")
        print("\nğŸ’¡ Vocabulary Ø¨Ø§ÛŒØ¯ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø³Ø§Ø®ØªÙ‡ Ø´ÙˆØ¯. Ù„Ø·ÙØ§Ù‹ Ø¯Ø³ØªÙˆØ± Ø²ÛŒØ± Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯:")
        print(f"   python main.py --data {args.data} --preprocess")
        return
    
    print(f"\nâœ“ ØªØ¹Ø¯Ø§Ø¯ Entities: {num_entities}")
    print(f"âœ“ ØªØ¹Ø¯Ø§Ø¯ Relations: {num_relations}")
    
    # Ø§ÛŒØ¬Ø§Ø¯ batch loaders Ø¨Ø±Ø§ÛŒ dev Ùˆ test
    dev_rank_batcher = StreamBatcher(
        args.data, 
        'dev_ranking', 
        args.test_batch_size, 
        randomize=False, 
        loader_threads=args.loader_threads, 
        keys=input_keys
    )
    
    test_rank_batcher = StreamBatcher(
        args.data, 
        'test_ranking', 
        args.test_batch_size, 
        randomize=False, 
        loader_threads=args.loader_threads, 
        keys=input_keys
    )
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„
    print(f"\nØ§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„: {args.model}")
    if args.model == 'conve':
        model = ConvE(args, num_entities, num_relations)
    elif args.model == 'distmult':
        model = DistMult(args, num_entities, num_relations)
    elif args.model == 'complex':
        model = Complex(args, num_entities, num_relations)
    else:
        raise Exception(f"Ù…Ø¯Ù„ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡: {args.model}")
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„
    print(f"\nØ¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø§Ø²: {args.model_path}")
    if not os.path.exists(args.model_path):
        print(f"Ø®Ø·Ø§: ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ ÛŒØ§ÙØª Ù†Ø´Ø¯: {args.model_path}")
        print("\nÙ…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù¾ÙˆØ´Ù‡ saved_models:")
        if os.path.exists('saved_models'):
            for f in os.listdir('saved_models'):
                if f.endswith('.model'):
                    print(f"  - {f}")
        return
    
    model_params = torch.load(args.model_path)
    model.load_state_dict(model_params)
    
    # Ø§Ù†ØªÙ‚Ø§Ù„ Ù…Ø¯Ù„ Ø¨Ù‡ GPU
    if args.cuda and torch.cuda.is_available():
        model.cuda()
        print("Ù…Ø¯Ù„ Ø¨Ù‡ GPU Ù…Ù†ØªÙ‚Ù„ Ø´Ø¯")
    else:
        print("Ù…Ø¯Ù„ Ø±ÙˆÛŒ CPU Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
    
    # ØªÙ†Ø¸ÛŒÙ… Ù…Ø¯Ù„ Ø¯Ø± Ø­Ø§Ù„Øª evaluation
    model.eval()
    
    # Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„
    print("\nØ§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„:")
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {total_params:,}")
    
    print("\nØ³Ø§Ø®ØªØ§Ø± Ù…Ø¯Ù„:")
    print(model)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„
    results_file = f"test_results_{args.data}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    
    # ØªØ³Øª Ø±ÙˆÛŒ test set
    print("\n" + "=" * 70)
    print("Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ Test Set")
    print("=" * 70)
    with torch.no_grad():
        test_results = ranking_and_hits(model, test_rank_batcher, vocab, 'Test Evaluation')
    
    # ØªØ³Øª Ø±ÙˆÛŒ dev set
    print("\n" + "=" * 70)
    print("Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ Dev Set")
    print("=" * 70)
    with torch.no_grad():
        dev_results = ranking_and_hits(model, dev_rank_batcher, vocab, 'Dev Evaluation')
    
    print("\n" + "=" * 70)
    print("ØªØ³Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!")
    print(f"Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„ log Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª")
    print("=" * 70)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ØªØ³Øª Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Knowledge Graph')
    
    # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„
    parser.add_argument('--model', type=str, default='conve', 
                        help='Ù†ÙˆØ¹ Ù…Ø¯Ù„: {conve, distmult, complex}')
    parser.add_argument('--data', type=str, default='FB15k-237', 
                        help='Dataset: {FB15k-237, YAGO3-10, WN18RR, umls, nations, kinship}')
    parser.add_argument('--model-path', type=str, default='saved_models/FB15k-237_conve_0.2_0.3.model',
                        help='Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡')
    
    # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù…Ø¯Ù„ (Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ù‡Ù…Ø®ÙˆØ§Ù†ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯)
    parser.add_argument('--embedding-dim', type=int, default=200,
                        help='Ø¨Ø¹Ø¯ embedding (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 200)')
    parser.add_argument('--embedding-shape1', type=int, default=20,
                        help='Ø¨Ø¹Ø¯ Ø§ÙˆÙ„ embedding 2D (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 20)')
    parser.add_argument('--hidden-drop', type=float, default=0.3,
                        help='Dropout Ø¨Ø±Ø§ÛŒ hidden layer (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 0.3)')
    parser.add_argument('--input-drop', type=float, default=0.2,
                        help='Dropout Ø¨Ø±Ø§ÛŒ input embeddings (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 0.2)')
    parser.add_argument('--feat-drop', type=float, default=0.2,
                        help='Dropout Ø¨Ø±Ø§ÛŒ convolutional features (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 0.2)')
    parser.add_argument('--hidden-size', type=int, default=9728,
                        help='Ø§Ù†Ø¯Ø§Ø²Ù‡ hidden layer (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 9728)')
    parser.add_argument('--use-bias', action='store_true',
                        help='Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² bias Ø¯Ø± convolutional layer')
    
    # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ØªØ³Øª
    parser.add_argument('--test-batch-size', type=int, default=128,
                        help='Ø§Ù†Ø¯Ø§Ø²Ù‡ batch Ø¨Ø±Ø§ÛŒ ØªØ³Øª (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 128)')
    parser.add_argument('--loader-threads', type=int, default=4,
                        help='ØªØ¹Ø¯Ø§Ø¯ thread Ø¨Ø±Ø§ÛŒ batch loader (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 4)')
    parser.add_argument('--cuda', action='store_true', default=True,
                        help='Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CUDA')
    parser.add_argument('--seed', type=int, default=17,
                        help='random seed (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 17)')
    
    args = parser.parse_args()
    
    # ØªÙ†Ø¸ÛŒÙ… global config
    Config.backend = 'pytorch'
    Config.cuda = args.cuda and torch.cuda.is_available()
    Config.embedding_dim = args.embedding_dim
    
    # ØªÙ†Ø¸ÛŒÙ… random seed
    torch.manual_seed(args.seed)
    if args.cuda and torch.cuda.is_available():
        torch.cuda.manual_seed(args.seed)
    
    # Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øª
    load_and_test_model(args)
