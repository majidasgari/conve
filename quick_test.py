#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ØªØ³Øª Ø³Ø±ÛŒØ¹ Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­Ø¯ÙˆØ¯ samples Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø±Ø¹Øª cache
"""

import torch
import time
from model import ConvE
from spodernet.preprocessing.pipeline import Pipeline
from spodernet.preprocessing.batching import StreamBatcher
from spodernet.utils.global_config import Config
from conve_reranker import ConvEReranker

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
Config.backend = 'pytorch'
Config.cuda = True
Config.embedding_dim = 200

print("=" * 70)
print("Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Dataset Ùˆ Vocabulary")
print("=" * 70)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ vocabulary
input_keys = ['e1', 'rel', 'rel_eval', 'e2', 'e2_multi1', 'e2_multi2']
p = Pipeline('FB15k-237', keys=input_keys)

try:
    p.load_vocabs()
except:
    print("\nâš ï¸  Vocabulary outdated ÛŒØ§ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯!")
    import sys
    sys.exit(1)

vocab = p.state['vocab']
num_entities = vocab['e1'].num_token
num_relations = vocab['rel'].num_token

print(f"âœ“ ØªØ¹Ø¯Ø§Ø¯ Entities: {num_entities}")
print(f"âœ“ ØªØ¹Ø¯Ø§Ø¯ Relations: {num_relations}")

print("\n" + "=" * 70)
print("Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„")
print("=" * 70)

# Ø§ÛŒØ¬Ø§Ø¯ args object
class Args:
    embedding_dim = 200
    embedding_shape1 = 20
    hidden_drop = 0.3
    input_drop = 0.2
    feat_drop = 0.2
    hidden_size = 9728
    use_bias = False

args = Args()

# Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„
model = ConvE(args, num_entities, num_relations)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§
model_path = 'saved_models/FB15k-237_conve_0.2_0.3.model'
print(f"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø§Ø²: {model_path}")
model_params = torch.load(model_path, weights_only=False)
model.load_state_dict(model_params)

# Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ GPU
model.cuda()
model.eval()

print("\n" + "=" * 70)
print("ØªØ³Øª Ø³Ø±ÛŒØ¹ Ø¨Ø§ 10 Ù†Ù…ÙˆÙ†Ù‡ Ø§ÙˆÙ„")
print("=" * 70)

# Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© custom batcher Ú©Ù‡ ÙÙ‚Ø· 50 Ù†Ù…ÙˆÙ†Ù‡ Ø§ÙˆÙ„ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
class LimitedBatcher:
    def __init__(self, original_batcher, limit=50):
        self.original_batcher = original_batcher
        self.limit = limit
        self.count = 0
        
    def __iter__(self):
        self.count = 0
        self.iter = iter(self.original_batcher)
        return self
    
    def __next__(self):
        if self.count >= self.limit:
            raise StopIteration
        self.count += 1
        return next(self.iter)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ test batcher
test_rank_batcher = StreamBatcher(
    'FB15k-237', 
    'test_ranking', 
    128, 
    randomize=False, 
    loader_threads=4, 
    keys=input_keys
)

limited_batcher = LimitedBatcher(test_rank_batcher, limit=10)

# Ø§ÛŒØ¬Ø§Ø¯ reranker
print("\nØ§ÛŒØ¬Ø§Ø¯ Reranker Ø¨Ø§ BGE...")
reranker = ConvEReranker(
    model=model,
    vocab=vocab,
    use_gpu=True,
    k=10,  # Ú©Ø§Ù‡Ø´ k Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø³Ø±ÛŒØ¹ØªØ±
    st_model_name="BAAI/bge-m3",
    data_path="data/FB15k-237"
)

print(f"\nØ´Ø±ÙˆØ¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ 10 Ù†Ù…ÙˆÙ†Ù‡ Ø§ÙˆÙ„...")
start_time = time.time()

# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ re-ranking
with torch.no_grad():
    reranker.ranking_and_hits_with_reranking(limited_batcher, 'Quick Test (10 samples)')

end_time = time.time()
elapsed = end_time - start_time

print(f"\nâ±ï¸  Ø²Ù…Ø§Ù† Ú©Ù„: {elapsed:.2f} Ø«Ø§Ù†ÛŒÙ‡")
print(f"â±ï¸  Ø²Ù…Ø§Ù† Ù‡Ø± Ù†Ù…ÙˆÙ†Ù‡: {elapsed/10:.3f} Ø«Ø§Ù†ÛŒÙ‡")
print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ embeddings Ø¯Ø± cache: {len(reranker.embedding_cache)}")

print("\n" + "=" * 70)
print("ØªØ³Øª Ú©Ø§Ù…Ù„ Ø´Ø¯!")
print("=" * 70)
